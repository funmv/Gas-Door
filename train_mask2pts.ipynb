{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e4e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import (pad, to_tensor, normalize,\n",
    "                                               hflip, vflip, rotate, crop)\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.dataset import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from skorch.callbacks import Checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import random\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a2f9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hflip_coordinates(start, end, image_width=256):\n",
    "    \"\"\"\n",
    "    Apply horizontal flip to the coordinates of a line in an image.\n",
    "\n",
    "    Parameters:\n",
    "    - start: tuple, (x1, y1) coordinates of the line start point\n",
    "    - end: tuple, (x2, y2) coordinates of the line end point\n",
    "    - image_width: int, width of the image (default is 256)\n",
    "\n",
    "    Returns:\n",
    "    - new_start: tuple, new (x1, y1) coordinates after the horizontal flip\n",
    "    - new_end: tuple, new (x2, y2) coordinates after the horizontal flip\n",
    "    \"\"\"\n",
    "    new_x1 = image_width - 1 - start[0]\n",
    "    new_y1 = start[1]\n",
    "    new_x2 = image_width - 1 - end[0]\n",
    "    new_y2 = end[1]\n",
    "\n",
    "    new_start = (new_x1, new_y1)\n",
    "    new_end = (new_x2, new_y2)\n",
    "\n",
    "    return *new_start, *new_end\n",
    "\n",
    "\n",
    "def vflip_coordinates(start, end, image_height=256):\n",
    "    \"\"\"\n",
    "    Apply vertical flip to the coordinates of a line in an image and swap start and end points.\n",
    "\n",
    "    Parameters:\n",
    "    - start: tuple, (x1, y1) coordinates of the line start point\n",
    "    - end: tuple, (x2, y2) coordinates of the line end point\n",
    "    - image_height: int, height of the image (default is 256)\n",
    "\n",
    "    Returns:\n",
    "    - new_start: tuple, new (x1, y1) coordinates after the vertical flip\n",
    "    - new_end: tuple, new (x2, y2) coordinates after the vertical flip\n",
    "    \"\"\"\n",
    "    new_x1 = start[0]\n",
    "    new_y1 = image_height - 1 - start[1]\n",
    "    new_x2 = end[0]\n",
    "    new_y2 = image_height - 1 - end[1]\n",
    "\n",
    "    # Ensure coordinates are within the bounds of the image\n",
    "    new_y2 = max(0, new_y2)\n",
    "\n",
    "    # Swap the coordinates\n",
    "    new_start = (new_x2, new_y2)\n",
    "    new_end = (new_x1, new_y1)\n",
    "\n",
    "    return *new_start, *new_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4392ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet50 모델을 정의 (특징 추출기 부분만 사용)\n",
    "class ResNet50FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50FeatureExtractor, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet50.children())[:-2])  # 마지막 두 층 제외\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "    \n",
    "# 특징 결합 및 회귀를 위한 모델 정의\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.feature_extractor1 = ResNet50FeatureExtractor()\n",
    "        self.feature_extractor2 = ResNet50FeatureExtractor()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2 * 2048 * 7 * 7, 512),  # 두 특징 맵을 결합한 후 FC 레이어에 전달\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4)  # (x1, y1, x2, y2) 출력\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        image1, image2 = x\n",
    "        features1 = self.feature_extractor1(image1)\n",
    "        features2 = self.feature_extractor2(image2)\n",
    "        combined_features = torch.cat((features1, features2), dim=1)  # 특징 결합\n",
    "        combined_features = combined_features.view(combined_features.size(0), -1)  # FC 레이어에 전달하기 위해 펼치기\n",
    "        output = self.fc(combined_features)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class ResNet18Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18Regressor, self).__init__()\n",
    "        resnet18 = models.resnet18(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(resnet18.children())[:-2])  # 마지막 두 층 제외\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4)  \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #image1, image2 = x\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "140f2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 함수 정의\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, data_sample, random_flips=False):\n",
    "        self.image_names = data_sample['Filename']\n",
    "        self.targets = data_sample['Longest_Line']\n",
    "        self.random_flips = random_flips\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join('crop_data', 'image', self.image_names[idx])\n",
    "        mask_path = os.path.join('crop_data', 'mask', self.image_names[idx])\n",
    "        image1 = preprocess_image(image_path)\n",
    "        mask = preprocess_image(mask_path)\n",
    "        pts = np.array(ast.literal_eval(self.targets[idx]), dtype=np.float32) \n",
    "        if self.random_flips:\n",
    "            if random.random() < 0.5:\n",
    "                #cell = hflip(cell)\n",
    "                mask = hflip(mask)\n",
    "                pts = [hflip_coordinates(pt[:2],pt[2:]) for pt in pts]\n",
    "\n",
    "            if random.random() < 0.5:\n",
    "                #cell = vflip(cell)\n",
    "                mask = vflip(mask)\n",
    "                pts = [vflip_coordinates(pt[:2],pt[2:]) for pt in pts]    \n",
    "                \n",
    "        pts = np.array([np.array(pt,dtype=np.float32)/255.0 for pt in pts], dtype=np.float32)\n",
    "        #pdb.set_trace()\n",
    "        #return (image1, image2), target #, self.image_names[idx]\n",
    "        return mask, pts #, self.image_names[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41c70661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]),\n",
       " array([[0.10196079, 0.        , 0.07843138, 1.0039216 ]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18Regressor()\n",
    "train_ds = ImagePairDataset(result_df, random_flips=True)\n",
    "mask, target = train_ds[0]\n",
    "lines = [vv * 224 for vv in target]  #target * 224.0  #\n",
    "show_tensor_image(mask, lines)\n",
    "mask.shape, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16fcb44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 학습 및 검증 데이터셋으로 분할\n",
    "# train_size = int(0.8 * len(train_ds))\n",
    "# val_size = len(train_ds) - train_size\n",
    "# train_ds, val_ds = random_split(train_ds, [train_size, val_size])\n",
    "\n",
    "# # 데이터 로더 정의\n",
    "# train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, pin_memory=True)\n",
    "# val_loader = DataLoader(val_ds, batch_size=8, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8291887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skorch를 사용하여 모델 정의 및 학습\n",
    "net = NeuralNetRegressor(\n",
    "    model, \n",
    "    max_epochs=500,\n",
    "    lr=0.001,\n",
    "    criterion=nn.MSELoss,\n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    iterator_valid__shuffle=False,    \n",
    "#     train_split=None,  # 내부 데이터 분할 비활성화\n",
    "#     iterator_train=train_loader,\n",
    "#     iterator_valid=val_loader,\n",
    "    callbacks=[Checkpoint(f_params='best_params.pt')],    \n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e9a971f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\2023\\venv\\nox39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([128, 1, 4])) that is different to the input size (torch.Size([128, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "D:\\2023\\venv\\nox39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 1, 4])) that is different to the input size (torch.Size([1, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1        \u001b[36m0.1575\u001b[0m        \u001b[32m0.0933\u001b[0m     +  5.1670\n",
      "      2        \u001b[36m0.0812\u001b[0m        \u001b[32m0.0838\u001b[0m     +  4.7819\n",
      "      3        \u001b[36m0.0802\u001b[0m        \u001b[32m0.0792\u001b[0m     +  4.9297\n",
      "      4        \u001b[36m0.0792\u001b[0m        \u001b[32m0.0759\u001b[0m     +  4.9458\n",
      "      5        \u001b[36m0.0777\u001b[0m        \u001b[32m0.0731\u001b[0m     +  5.0774\n",
      "      6        0.0789        \u001b[32m0.0729\u001b[0m     +  5.0742\n",
      "      7        \u001b[36m0.0774\u001b[0m        \u001b[32m0.0724\u001b[0m     +  5.0462\n",
      "      8        0.0822        0.0805        4.8793\n",
      "      9        0.0794        \u001b[32m0.0717\u001b[0m     +  4.8195\n",
      "     10        0.0781        0.0781        5.0378\n",
      "     11        0.0809        0.0725        4.9833\n",
      "     12        0.0776        \u001b[32m0.0714\u001b[0m     +  4.9818\n",
      "     13        0.0790        0.0741        5.1139\n",
      "     14        0.0792        0.0922        4.8904\n",
      "     15        0.0954        0.0787        4.8868\n",
      "     16        0.0805        0.0750        5.0325\n",
      "     17        0.0785        0.0714        4.6759\n",
      "     18        0.0774        0.0742        4.6833\n",
      "     19        0.0825        0.0737        4.6481\n",
      "     20        0.0892        0.0924        4.6753\n",
      "     21        0.0912        0.1245        4.7896\n",
      "     22        0.0927        0.0770        4.7104\n",
      "     23        0.0790        0.0731        4.7911\n",
      "     24        0.0823        0.0722        4.7267\n",
      "     25        0.0837        0.0752        4.9486\n",
      "     26        0.0872        0.0723        5.1278\n",
      "     27        0.0867        0.0761        4.9448\n",
      "     28        0.0846        0.0785        4.8540\n",
      "     29        0.0821        0.0731        4.8509\n",
      "     30        0.0783        \u001b[32m0.0713\u001b[0m     +  4.8247\n",
      "     31        0.0789        0.0718        5.0351\n",
      "     32        0.0791        0.0719        5.0471\n",
      "     33        0.0775        \u001b[32m0.0709\u001b[0m     +  5.0074\n",
      "     34        \u001b[36m0.0767\u001b[0m        \u001b[32m0.0704\u001b[0m     +  5.0778\n",
      "     35        0.0778        0.0728        4.8793\n",
      "     36        0.0774        0.0755        4.7795\n",
      "     37        0.0791        0.0770        4.9471\n",
      "     38        0.0807        \u001b[32m0.0700\u001b[0m     +  4.8965\n",
      "     39        0.0769        0.0705        4.8986\n",
      "     40        0.0782        0.0708        4.7742\n",
      "     41        0.0771        0.0706        4.7473\n",
      "     42        0.0773        0.0732        4.7848\n",
      "     43        \u001b[36m0.0765\u001b[0m        0.0736        4.7660\n",
      "     44        0.0813        0.0737        4.8074\n",
      "     45        0.0794        0.0711        4.7390\n",
      "     46        0.0787        0.0729        4.7795\n",
      "     47        0.0828        0.0734        4.7911\n",
      "     48        0.0834        0.0796        4.7983\n",
      "     49        0.0793        0.0717        4.7787\n",
      "     50        0.0779        0.0709        4.8040\n",
      "     51        0.0767        0.0705        4.7596\n",
      "     52        0.0776        0.0710        4.7832\n",
      "     53        0.0768        0.0715        4.7839\n",
      "     54        0.0766        0.0707        4.7650\n",
      "     55        \u001b[36m0.0763\u001b[0m        0.0701        4.7324\n",
      "     56        0.0776        0.0719        4.8319\n",
      "     57        0.0775        0.0727        4.8106\n",
      "     58        0.0775        0.0713        4.8016\n",
      "     59        0.0791        0.0713        4.7819\n",
      "     60        0.0771        0.0709        4.7665\n",
      "     61        \u001b[36m0.0757\u001b[0m        0.0708        4.8017\n",
      "     62        0.0760        0.0702        4.8031\n",
      "     63        \u001b[36m0.0757\u001b[0m        0.0718        4.7663\n",
      "     64        0.0765        0.0721        4.7515\n",
      "     65        0.0776        0.0705        4.8008\n",
      "     66        0.0767        0.0715        4.8050\n",
      "     67        0.0791        0.0711        4.8279\n",
      "     68        0.0771        0.0737        4.8351\n",
      "     69        0.0779        0.0715        4.7383\n",
      "     70        0.0763        0.0704        4.8003\n",
      "     71        0.0779        0.0713        4.7856\n",
      "     72        0.0776        \u001b[32m0.0699\u001b[0m     +  4.7760\n",
      "     73        0.0759        0.0709        4.9021\n",
      "     74        0.0777        0.0718        4.7902\n",
      "     75        0.0779        0.0703        4.8108\n",
      "     76        0.0762        0.0723        4.8042\n",
      "     77        0.0772        0.0718        4.7932\n",
      "     78        0.0766        0.0712        4.8055\n",
      "     79        0.0769        0.0799        4.9410\n",
      "     80        0.0771        0.0712        4.8602\n",
      "     81        0.0768        0.0728        4.8699\n",
      "     82        0.0796        0.0720        4.8587\n",
      "     83        0.0798        0.0753        4.8568\n",
      "     84        0.0774        0.0728        4.8856\n",
      "     85        0.0767        0.0704        4.9204\n",
      "     86        0.0767        0.0714        4.9282\n",
      "     87        0.0767        0.0717        4.8349\n",
      "     88        0.0766        0.0724        4.8261\n",
      "     89        0.0778        0.0747        4.8030\n",
      "     90        0.0780        0.0707        4.7794\n",
      "     91        0.0768        0.0703        4.8324\n",
      "     92        0.0772        0.0707        4.8454\n",
      "     93        0.0758        0.0733        4.8153\n",
      "     94        0.0783        0.0752        4.7648\n",
      "     95        0.0761        0.0732        4.7946\n",
      "     96        0.0757        0.0741        4.8126\n",
      "     97        0.0766        0.0722        4.8591\n",
      "     98        0.0760        0.0715        4.7836\n",
      "     99        0.0762        0.0718        4.8411\n",
      "    100        0.0761        0.0727        4.8287\n",
      "    101        0.0833        0.0755        4.8383\n",
      "    102        0.0773        0.0762        4.8438\n",
      "    103        0.0790        0.0722        4.8542\n",
      "    104        0.0768        0.0713        4.8762\n",
      "    105        0.0768        0.0722        4.8292\n",
      "    106        0.0794        0.0731        4.8438\n",
      "    107        0.0789        0.0747        4.7911\n",
      "    108        0.0828        0.0983        4.8775\n",
      "    109        0.0794        0.0794        4.8459\n",
      "    110        0.0859        0.0791        4.8383\n",
      "    111        0.0788        0.0713        4.8609\n",
      "    112        0.0776        0.0734        4.8809\n",
      "    113        0.0790        0.0777        4.8103\n",
      "    114        0.0805        0.0743        4.8319\n",
      "    115        0.0793        0.0736        4.8446\n",
      "    116        0.0786        0.0700        4.8344\n",
      "    117        0.0778        0.0715        4.8391\n",
      "    118        0.0765        0.0724        4.8588\n",
      "    119        0.0780        0.0705        4.8339\n",
      "    120        0.0773        0.0725        4.8749\n",
      "    121        0.0769        \u001b[32m0.0698\u001b[0m     +  4.8688\n",
      "    122        0.0783        0.0710        4.9355\n",
      "    123        0.0781        0.0723        4.8994\n",
      "    124        0.0780        0.0750        4.8684\n",
      "    125        \u001b[36m0.0756\u001b[0m        0.0708        4.8313\n",
      "    126        0.0765        0.0707        4.8666\n",
      "    127        0.0765        0.0707        4.8075\n",
      "    128        0.0756        0.0720        4.8997\n",
      "    129        0.0761        0.0707        4.8618\n",
      "    130        0.0761        0.0711        4.9275\n",
      "    131        0.0761        0.0713        4.9001\n",
      "    132        0.0763        0.0712        4.8881\n",
      "    133        0.0758        0.0765        4.8473\n",
      "    134        0.0807        0.0752        4.8496\n",
      "    135        0.0792        0.0708        4.9555\n",
      "    136        0.0777        0.0704        4.8755\n",
      "    137        0.0767        0.0704        4.8682\n",
      "    138        0.0760        0.0706        4.8334\n",
      "    139        0.0765        0.0704        4.8878\n",
      "    140        0.0768        0.0741        4.8159\n",
      "    141        0.0765        0.0754        4.8492\n",
      "    142        0.0776        0.0763        4.8270\n",
      "    143        0.0922        0.0756        4.8751\n",
      "    144        0.0841        0.1234        4.8953\n",
      "    145        0.0785        0.0843        4.9966\n",
      "    146        0.0765        0.0790        4.8926\n",
      "    147        0.0766        0.0736        4.8950\n",
      "    148        0.0807        0.0710        4.8664\n",
      "    149        0.0771        0.0732        4.8684\n",
      "    150        0.0768        0.0717        4.8949\n",
      "    151        0.0759        0.0712        4.9071\n",
      "    152        0.0777        0.0806        4.8684\n",
      "    153        0.0890        0.0993        4.8554\n",
      "    154        0.0825        0.0737        4.9013\n",
      "    155        0.0771        0.0712        4.8942\n",
      "    156        0.0773        0.0705        4.8710\n",
      "    157        0.0770        0.0705        4.8338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    158        0.0760        0.0713        4.9033\n",
      "    159        0.0762        0.0702        4.9692\n",
      "    160        0.0762        0.0708        4.8673\n",
      "    161        0.0762        0.0736        4.8466\n",
      "    162        0.0760        0.0708        4.8665\n",
      "    163        0.0762        0.0704        4.8865\n",
      "    164        0.0773        0.0740        4.9076\n",
      "    165        0.0764        0.0730        4.9086\n",
      "    166        0.0765        0.0702        4.8653\n",
      "    167        0.0763        0.0709        4.8809\n",
      "    168        0.0763        0.0710        4.8466\n",
      "    169        0.0759        0.0709        4.8278\n",
      "    170        0.0762        0.0705        4.9332\n",
      "    171        \u001b[36m0.0753\u001b[0m        0.0720        4.9198\n",
      "    172        0.0777        0.0720        4.8137\n",
      "    173        0.0763        0.0702        4.8795\n",
      "    174        0.0764        0.0720        4.8757\n",
      "    175        0.0787        0.0753        4.8822\n",
      "    176        0.0777        0.0713        4.8376\n",
      "    177        0.0763        0.0704        4.8689\n",
      "    178        0.0759        0.0709        4.8579\n",
      "    179        0.0779        0.0708        4.9054\n",
      "    180        0.0767        0.0709        4.9269\n",
      "    181        0.0777        0.0730        4.9053\n",
      "    182        0.0766        0.0703        4.9333\n",
      "    183        0.0761        0.0707        4.8674\n",
      "    184        0.0759        0.0716        4.8833\n",
      "    185        0.0775        0.0759        4.9458\n",
      "    186        0.0764        \u001b[32m0.0694\u001b[0m     +  4.9201\n",
      "    187        0.0765        0.0773        5.0225\n",
      "    188        0.0855        0.0776        4.8559\n",
      "    189        0.0784        0.0710        4.8877\n",
      "    190        0.0774        0.0708        4.9711\n",
      "    191        0.0760        0.0707        4.9120\n",
      "    192        0.0760        0.0706        4.8385\n",
      "    193        0.0761        0.0706        4.9369\n",
      "    194        0.0767        0.0711        4.9202\n",
      "    195        0.0766        0.0719        4.9082\n",
      "    196        0.0763        0.0717        4.9363\n",
      "    197        0.0770        0.0743        4.8708\n",
      "    198        0.0773        0.0713        4.8953\n",
      "    199        0.0773        0.0707        4.9394\n",
      "    200        0.0765        0.0724        4.9267\n",
      "    201        0.0765        0.0719        4.9350\n",
      "    202        0.0759        0.0703        4.9356\n",
      "    203        0.0773        0.0714        4.9392\n",
      "    204        0.0768        0.0704        4.9654\n",
      "    205        0.0765        0.0706        4.9161\n",
      "    206        0.0759        0.0712        4.8832\n",
      "    207        0.0759        0.0728        4.9068\n",
      "    208        0.0759        0.0704        4.9533\n",
      "    209        0.0776        0.0722        4.9807\n",
      "    210        0.0762        0.0713        4.8811\n",
      "    211        0.0764        0.0703        4.8660\n",
      "    212        0.0765        0.0724        4.9025\n",
      "    213        0.0804        0.0760        4.9671\n",
      "    214        0.0797        0.0702        4.9178\n",
      "    215        0.0772        0.0716        4.9381\n",
      "    216        0.0764        0.0706        4.9231\n",
      "    217        0.0790        0.0762        4.9429\n",
      "    218        0.0783        0.0705        4.8851\n",
      "    219        0.0785        0.0709        4.8928\n",
      "    220        0.0765        0.0721        4.9057\n",
      "    221        0.0766        0.0723        4.9064\n",
      "    222        0.0759        0.0767        4.9358\n",
      "    223        0.0762        0.0709        4.8853\n",
      "    224        0.0757        0.0710        4.9036\n",
      "    225        0.0757        0.0705        4.9448\n",
      "    226        0.0765        0.0705        4.9531\n",
      "    227        0.0770        0.0740        4.9115\n",
      "    228        0.0771        0.0744        4.9516\n",
      "    229        0.0764        0.0727        4.9314\n",
      "    230        0.0799        0.0720        5.0495\n",
      "    231        0.0776        0.0706        4.9594\n",
      "    232        0.0761        0.0705        4.9667\n",
      "    233        0.0756        0.0700        4.9195\n",
      "    234        0.0762        0.0703        4.9162\n",
      "    235        0.0763        0.0712        4.9789\n",
      "    236        0.0778        0.0707        4.9810\n",
      "    237        0.0768        0.0708        4.8847\n",
      "    238        0.0767        0.0718        4.9579\n",
      "    239        0.0762        0.0703        4.9681\n",
      "    240        0.0760        0.0698        4.9455\n",
      "    241        0.0766        0.0697        4.9523\n",
      "    242        0.0775        0.0750        4.9350\n",
      "    243        0.0790        0.0763        4.8811\n",
      "    244        0.0773        0.0718        4.9237\n",
      "    245        0.0765        0.0713        4.9516\n",
      "    246        0.0760        0.0732        4.9069\n",
      "    247        0.0775        0.0736        5.0041\n",
      "    248        0.0759        0.0704        4.9602\n",
      "    249        0.0758        0.0703        4.9415\n",
      "    250        0.0761        0.0703        4.9245\n",
      "    251        0.0760        0.0715        4.9222\n",
      "    252        \u001b[36m0.0753\u001b[0m        0.0723        4.9821\n",
      "    253        0.0753        0.0728        4.9584\n",
      "    254        0.0760        0.0703        4.9133\n",
      "    255        0.0759        0.0707        4.9739\n",
      "    256        0.0756        0.0704        4.9345\n",
      "    257        0.0759        0.0703        4.9309\n",
      "    258        0.0755        0.0696        4.9164\n",
      "    259        0.0761        0.0713        5.0244\n",
      "    260        0.0760        0.0714        4.8898\n",
      "    261        0.0767        0.0733        4.9343\n",
      "    262        0.0813        0.0715        4.8794\n",
      "    263        0.0804        0.0723        4.9477\n",
      "    264        0.0782        0.0716        4.9744\n",
      "    265        0.0778        0.0712        4.9794\n",
      "    266        0.0773        0.0720        4.9824\n",
      "    267        0.0769        0.0717        5.0214\n",
      "    268        0.0764        0.0730        4.9257\n",
      "    269        0.0762        0.0711        5.0097\n",
      "    270        0.0760        0.0704        4.9570\n",
      "    271        0.0763        0.0708        4.9389\n",
      "    272        0.0766        0.0717        4.8852\n",
      "    273        0.0762        0.0706        4.9884\n",
      "    274        0.0760        0.0705        4.9156\n",
      "    275        0.0762        0.0703        4.9198\n",
      "    276        0.0763        0.0713        4.9402\n",
      "    277        0.0764        0.0706        4.9456\n",
      "    278        0.0757        0.0702        4.9463\n",
      "    279        0.0756        0.0701        4.9939\n",
      "    280        0.0770        0.0703        4.9486\n",
      "    281        0.0762        0.0723        4.9084\n",
      "    282        0.0763        0.0717        5.0417\n",
      "    283        0.0755        0.0734        4.9101\n",
      "    284        0.0765        0.0712        4.9928\n",
      "    285        0.0767        0.0728        4.9626\n",
      "    286        0.0764        0.0706        4.9700\n",
      "    287        0.0766        0.0743        5.0048\n",
      "    288        0.0767        0.0708        5.0223\n",
      "    289        0.0759        0.0711        4.9935\n",
      "    290        0.0758        0.0728        4.9876\n",
      "    291        0.0807        0.0722        4.9559\n",
      "    292        0.0772        0.0712        4.9538\n",
      "    293        0.0761        0.0719        4.9700\n",
      "    294        0.0769        0.0710        4.9357\n",
      "    295        0.0775        0.0715        4.9617\n",
      "    296        0.0757        0.0766        5.0072\n",
      "    297        0.0760        0.0708        4.9486\n",
      "    298        0.0759        0.0699        4.9310\n",
      "    299        0.0761        0.0705        4.9544\n",
      "    300        0.0754        0.0702        4.9975\n",
      "    301        0.0759        0.0713        4.9648\n",
      "    302        0.0779        0.0726        4.9945\n",
      "    303        0.0771        0.0709        4.9591\n",
      "    304        0.0758        0.0707        4.9918\n",
      "    305        0.0758        0.0712        4.9736\n",
      "    306        0.0759        0.0729        5.0172\n",
      "    307        0.0758        0.0749        4.9228\n",
      "    308        0.0761        0.0716        4.9064\n",
      "    309        0.0761        0.0705        4.9801\n",
      "    310        0.0758        0.0717        5.0182\n",
      "    311        0.0759        0.0722        4.9324\n",
      "    312        0.0762        0.0746        4.9381\n",
      "    313        0.0760        0.0704        4.9370\n",
      "    314        0.0755        0.0708        4.9712\n",
      "    315        0.0759        0.0706        5.0220\n",
      "    316        0.0761        0.0703        5.0017\n",
      "    317        0.0758        0.0759        4.9827\n",
      "    318        0.0759        0.0705        4.9406\n",
      "    319        0.0759        0.0704        4.9628\n",
      "    320        0.0759        0.0720        4.9281\n",
      "    321        0.0755        0.0717        5.0087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    322        0.0753        0.0712        4.9462\n",
      "    323        0.0761        0.0741        4.9821\n",
      "    324        0.0757        0.0711        5.0587\n",
      "    325        0.0760        0.0706        4.9619\n",
      "    326        0.0757        0.0726        5.0131\n",
      "    327        0.0759        0.0705        4.9565\n",
      "    328        \u001b[36m0.0753\u001b[0m        0.0706        4.9766\n",
      "    329        0.0757        0.0744        4.9887\n",
      "    330        0.0757        0.0704        5.0373\n",
      "    331        0.0764        0.0705        4.9191\n",
      "    332        0.0764        0.0748        4.9919\n",
      "    333        0.0768        0.0704        4.9777\n",
      "    334        0.0761        0.0703        5.0026\n",
      "    335        0.0760        0.0702        4.9917\n",
      "    336        0.0765        0.0718        5.0178\n",
      "    337        0.0760        0.0699        5.0625\n",
      "    338        \u001b[36m0.0752\u001b[0m        0.0700        5.0085\n",
      "    339        0.0762        0.0718        4.9873\n",
      "    340        0.0768        0.0714        5.0071\n",
      "    341        0.0763        0.0703        4.9545\n",
      "    342        0.0759        0.0709        5.0559\n",
      "    343        0.0801        0.0739        4.9872\n",
      "    344        0.0779        0.0709        5.0086\n",
      "    345        0.0769        0.0730        4.9597\n",
      "    346        0.0760        0.0721        4.9657\n",
      "    347        0.0763        0.0710        4.9664\n",
      "    348        0.0760        0.0740        5.0202\n",
      "    349        0.0757        0.0717        4.9835\n",
      "    350        0.0785        0.0721        4.9664\n",
      "    351        0.0771        0.0724        4.9470\n",
      "    352        0.0777        0.0705        5.0137\n",
      "    353        0.0761        0.0711        5.0395\n",
      "    354        0.0788        0.0734        5.0022\n",
      "    355        0.0774        0.0718        4.9977\n",
      "    356        0.0766        0.0737        5.0653\n",
      "    357        0.0774        0.0754        5.0214\n",
      "    358        0.0770        0.0749        4.9705\n",
      "    359        0.0787        0.0730        5.0196\n",
      "    360        0.0770        0.0708        4.9978\n",
      "    361        0.0774        0.0727        4.9716\n",
      "    362        0.0773        0.0714        4.9693\n",
      "    363        0.0764        0.0712        4.9973\n",
      "    364        0.0769        0.0727        5.0296\n",
      "    365        0.0779        0.0713        5.0399\n",
      "    366        0.0769        0.0711        4.9873\n",
      "    367        0.0765        0.0707        4.9999\n",
      "    368        0.0761        0.0709        5.0385\n",
      "    369        0.0765        0.0704        4.9764\n",
      "    370        0.0761        0.0702        5.0160\n",
      "    371        0.0760        0.0708        4.9916\n",
      "    372        0.0758        0.0708        4.9761\n",
      "    373        0.0760        0.0714        5.0100\n",
      "    374        0.0757        0.0716        5.0057\n",
      "    375        0.0754        0.0709        5.0023\n",
      "    376        0.0759        0.0710        4.9575\n",
      "    377        0.0771        0.0714        5.0271\n",
      "    378        0.0761        0.0718        5.0525\n",
      "    379        0.0759        0.0724        5.0078\n",
      "    380        0.0761        0.0705        4.9926\n",
      "    381        0.0756        0.0709        5.0022\n",
      "    382        0.0755        0.0739        4.9741\n",
      "    383        0.0754        0.0751        4.9475\n",
      "    384        0.0927        0.0868        4.9925\n",
      "    385        0.0825        0.1101        4.9671\n",
      "    386        0.0790        0.0752        4.9982\n",
      "    387        0.0761        0.0716        4.9750\n",
      "    388        0.0761        0.0699        4.9881\n",
      "    389        0.0758        0.0699        5.0076\n",
      "    390        0.0764        0.0705        4.9432\n",
      "    391        0.0759        0.0748        5.0168\n",
      "    392        0.0757        0.0716        4.9936\n",
      "    393        0.0756        0.0704        4.9788\n",
      "    394        0.0758        0.0711        4.9887\n",
      "    395        0.0754        0.0700        4.9501\n",
      "    396        0.0756        0.0709        5.0346\n",
      "    397        0.0754        0.0704        5.0298\n",
      "    398        0.0760        0.0702        5.0172\n",
      "    399        0.0755        0.0703        5.0064\n",
      "    400        0.0758        0.0701        4.9942\n",
      "    401        0.0758        0.0711        4.9818\n",
      "    402        0.0755        0.0717        4.9763\n",
      "    403        0.0763        0.0705        4.9638\n",
      "    404        0.0758        0.0701        4.9952\n",
      "    405        0.0759        0.0704        5.0485\n",
      "    406        0.0759        0.0711        5.0001\n",
      "    407        0.0761        0.0728        4.9626\n",
      "    408        0.0757        0.0737        5.0089\n",
      "    409        0.0758        0.0721        5.0370\n",
      "    410        0.0776        0.0731        5.0173\n",
      "    411        0.0766        0.0712        4.9731\n",
      "    412        0.0759        0.0705        4.9900\n",
      "    413        0.0757        0.0706        4.9745\n",
      "    414        0.0757        0.0706        4.9919\n",
      "    415        0.0771        0.0719        5.0796\n",
      "    416        0.0778        0.0749        5.0090\n",
      "    417        0.0826        0.0741        4.9751\n",
      "    418        0.0786        0.0711        4.9784\n",
      "    419        0.0785        0.0712        5.0136\n",
      "    420        0.0773        0.0718        5.0219\n",
      "    421        0.0763        0.0710        4.9752\n",
      "    422        0.0763        0.0722        5.0096\n",
      "    423        0.0765        0.0708        5.0039\n",
      "    424        0.0763        0.0701        5.0019\n",
      "    425        0.0764        0.0705        4.9917\n",
      "    426        0.0764        0.0736        5.0140\n",
      "    427        0.0760        0.0719        4.9605\n",
      "    428        0.0759        0.0707        4.9697\n",
      "    429        0.0756        0.0706        5.0090\n",
      "    430        0.0759        0.0714        5.1108\n",
      "    431        0.0769        0.0755        4.9789\n",
      "    432        0.0809        0.0712        5.0192\n",
      "    433        0.0783        0.0717        4.9996\n",
      "    434        0.0839        0.0735        4.9837\n",
      "    435        0.0788        0.0817        5.0787\n",
      "    436        0.0780        0.0753        5.0037\n",
      "    437        0.0764        0.0734        4.9918\n",
      "    438        0.0770        0.0742        5.0407\n",
      "    439        0.0763        0.0706        4.9877\n",
      "    440        0.0762        0.0717        4.9853\n",
      "    441        0.0762        0.0713        5.0015\n",
      "    442        0.0755        0.0708        4.9757\n",
      "    443        0.0758        0.0703        5.0259\n",
      "    444        0.0759        0.0725        4.9886\n",
      "    445        0.0758        0.0731        5.0075\n",
      "    446        0.0760        0.0718        5.0841\n",
      "    447        0.0812        0.0739        5.0222\n",
      "    448        0.0780        0.0776        4.9949\n",
      "    449        0.0774        0.0716        5.2298\n",
      "    450        0.0766        0.0714        5.3375\n",
      "    451        0.0760        0.0705        5.1281\n",
      "    452        0.0762        0.0717        5.0341\n",
      "    453        0.0764        0.0733        5.0385\n",
      "    454        0.0760        0.0710        5.0789\n",
      "    455        0.0757        0.0707        5.2099\n",
      "    456        0.0792        0.0728        5.0964\n",
      "    457        0.0773        0.0743        5.1785\n",
      "    458        0.0766        0.0720        5.2101\n",
      "    459        0.0761        0.0698        5.1492\n",
      "    460        0.0761        0.0724        5.1249\n",
      "    461        0.0762        0.0749        5.0954\n",
      "    462        0.0761        0.0734        4.9630\n",
      "    463        0.0759        0.0704        4.9769\n",
      "    464        0.0755        0.0717        4.9906\n",
      "    465        0.0756        0.0724        5.0038\n",
      "    466        0.0760        0.0699        5.0156\n",
      "    467        0.0762        0.0706        5.0395\n",
      "    468        0.0756        0.0717        4.9873\n",
      "    469        0.0760        0.0697        5.0425\n",
      "    470        0.0758        0.0708        4.9810\n",
      "    471        0.0756        0.0703        4.9638\n",
      "    472        0.0759        0.0717        5.0310\n",
      "    473        0.0758        0.0709        5.0242\n",
      "    474        0.0762        0.0730        4.9538\n",
      "    475        0.0762        0.0719        4.9842\n",
      "    476        0.0758        0.0705        5.0345\n",
      "    477        0.0761        0.0707        5.0133\n",
      "    478        0.0756        0.0709        5.0068\n",
      "    479        0.0757        0.0706        5.0165\n",
      "    480        0.0760        0.0704        5.0098\n",
      "    481        0.0760        0.0702        4.9845\n",
      "    482        0.0763        0.0718        4.9819\n",
      "    483        0.0760        0.0704        5.0057\n",
      "    484        0.0757        0.0715        4.9928\n",
      "    485        0.0761        0.0711        4.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    486        0.0756        \u001b[32m0.0692\u001b[0m     +  5.0844\n",
      "    487        0.0757        0.0726        5.1366\n",
      "    488        0.0763        0.0696        5.0609\n",
      "    489        0.0758        0.0709        4.9957\n",
      "    490        0.0761        0.0707        4.9476\n",
      "    491        0.0757        0.0724        5.0188\n",
      "    492        0.0785        0.0743        5.0966\n",
      "    493        0.0780        0.0711        5.0197\n",
      "    494        0.0761        0.0734        5.0731\n",
      "    495        0.0758        0.0747        5.0211\n",
      "    496        0.0760        0.0703        4.9664\n",
      "    497        0.0759        0.0721        5.0062\n",
      "    498        0.0762        0.0713        5.0283\n",
      "    499        0.0762        0.0726        5.0097\n",
      "    500        0.0759        0.0719        4.9779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=ResNet18Regressor(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋으로 모델 학습\n",
    "net.fit(train_ds, y=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99473689",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa=net.predict(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef10210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensor_image, lines=None):\n",
    "    \"\"\"\n",
    "    Display a tensor image with optional lines.\n",
    "    \n",
    "    Parameters:\n",
    "    - tensor_image: torch.Tensor, image tensor of shape (3, 256, 256)\n",
    "    - lines: list of tuples, each containing start and end points of a line ((x1, y1), (x2, y2))\n",
    "    \"\"\"\n",
    "    np_image = tensor_image.numpy()\n",
    "\n",
    "    # Transpose the image to (256, 256, 3) from (3, 256, 256)\n",
    "    np_image = np.transpose(np_image, (1, 2, 0))\n",
    "    \n",
    "    np_image = (np_image - np_image.min()) / (np_image.max() - np_image.min())\n",
    "    plt.imshow(np_image)\n",
    "\n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            start, end = line[:2],line[2:]\n",
    "            x_values = [start[0], end[0]]\n",
    "            y_values = [start[1], end[1]]\n",
    "            plt.plot(x_values, y_values, 'r')  # 'r' is the color red\n",
    "    \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def show_numpy_image(np_image, lines=None):\n",
    "    # Normalize the image to range [0, 1] if necessary\n",
    "    np_image = (np_image - np_image.min()) / (np_image.max() - np_image.min())\n",
    "    \n",
    "    # Display the image\n",
    "    #plt.imshow(np_image, cmap='gray') \n",
    "    plt.imshow(np_image) \n",
    "    \n",
    "    if lines is not None:\n",
    "        for line in lines:\n",
    "            start, end = line[:2],line[2:]\n",
    "            x_values = [start[0], end[0]]\n",
    "            y_values = [start[1], end[1]]\n",
    "            plt.plot(x_values, y_values, 'r')  # 'r' is the color red\n",
    "    \n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image1, image2), target = train_ds[0]\n",
    "#print(target*255, fname)\n",
    "#lines = [int(x * 224) for x in target.squeeze()]\n",
    "out = model((image1.cuda().unsqueeze(0), image2.cuda().unsqueeze(0)))\n",
    "print(out, target)\n",
    "show_tensor_image(image2, target*224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dced95",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa.shape, len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6db49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image1, image2), target, fname = train_ds[0]\n",
    "print(target*255, fname)\n",
    "lines = [int(x * 255) for x in target]\n",
    "show_tensor_image(image2, target*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0372011e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90716d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8c008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76bec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Filename          Longest_Line\n",
      "0    frame_0089_part_1_time_41.53.jpg  [(229, 0, 235, 256)]\n",
      "1  frame_0089_part_1_time_41.53_1.jpg    [(54, 0, 62, 256)]\n",
      "2  frame_0089_part_1_time_41.53_2.jpg  [(238, 0, 244, 256)]\n",
      "frame_0089_part_1_time_41.53.jpg\n"
     ]
    }
   ],
   "source": [
    "# CSV 파일 읽기\n",
    "csv_path = os.path.join('crop_data', 'pts_all.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# 유클리드 거리 계산 함수\n",
    "def euclidean_distance(points):\n",
    "    x1, y1, x2, y2 = points\n",
    "    return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "\n",
    "# 가장 긴 직선만 남기기\n",
    "def get_longest_line(points_str):\n",
    "    points_list = ast.literal_eval(points_str)\n",
    "    if len(points_list) <= 1:\n",
    "        return points_str\n",
    "    distances = [euclidean_distance(points) for points in points_list]\n",
    "    max_distance_idx = np.argmax(distances)\n",
    "    longest_line = points_list[max_distance_idx]\n",
    "    return str([longest_line])\n",
    "\n",
    "# 각 행에 대해 가장 긴 직선만 남기기\n",
    "df['Longest_Line'] = df['Points'].apply(get_longest_line)\n",
    "\n",
    "# 필요한 열만 선택\n",
    "result_df = df[['Filename', 'Longest_Line']]\n",
    "\n",
    "# 결과 확인\n",
    "print(result_df[:3])\n",
    "#len(result_df), result_df['Filename']\n",
    "\n",
    "for fname in result_df['Filename']:\n",
    "    print(fname)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54391468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3761c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eb5ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
